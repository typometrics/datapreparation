{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine separate analyse files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ ['f.tsv', 'cat.tsv', 'positive-direction.tsv', 'cf.tsv', 'cfc-dist.tsv', 'cf-dist.tsv', 'cfc.tsv', 'f-dist-noroot.tsv', 'posdircf.tsv', 'posdircfc.tsv', 'fc.tsv', 'f-dist.tsv', 'abs-f-dist-noroot.tsv'] \n",
      " -----\n"
     ]
    }
   ],
   "source": [
    "folders = [\"sud2.7-analysis\",\"cs_pdt-analysis\", \"de-hdt-analysis\", \"ja_bccwj-analysis\",\"ru_syntagrus-analysis\"]\n",
    "#\"sud2.7-analysis\" contains the most numbers of col name\n",
    "\n",
    "outputFolder = \"sud-treebanks-v2.7-analysis\"\n",
    "filesname = os.listdir(folders[0])\n",
    "print(\"------\",filesname, \"\\n -----\")\n",
    "     \n",
    "for folder in folders:\n",
    "    ck = os.listdir(folder)\n",
    "    assert(ck == filesname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f.tsv\n",
      "cat.tsv\n",
      "positive-direction.tsv\n",
      "cf.tsv\n",
      "cfc-dist.tsv\n",
      "cf-dist.tsv\n",
      "cfc.tsv\n",
      "f-dist-noroot.tsv\n",
      "posdircf.tsv\n",
      "posdircfc.tsv\n",
      "fc.tsv\n",
      "f-dist.tsv\n",
      "abs-f-dist-noroot.tsv\n"
     ]
    }
   ],
   "source": [
    "#find column name list that covers all the column names of files with the same filename\n",
    "colNamesDict = {}\n",
    "for fl in filesname:\n",
    "    print(fl)\n",
    "    colNames=[]\n",
    "    for i in range(len(folders)):\n",
    "        #print(folders[i])\n",
    "        with open(folders[i]+'/'+fl, 'r+',encoding=\"utf8\") as f:\n",
    "            line0 = f.readline().strip()\n",
    "            cols = line0.split('\\t')\n",
    "            #print(cols, '\\n')\n",
    "            colNames+= cols[1:]\n",
    "            \n",
    "    colNamesDict[fl] = sorted(list(set(colNames)))\n",
    "    #print('\\n for file ', fl, \"colnames = \", colNamesDict[fl], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appos', 'cc', 'clf', 'comp', 'comp:aux', 'comp:cleft', 'comp:obj', 'comp:obl', 'comp:pred', 'compound', 'compound:prt', 'compound:redup', 'compound:svc', 'conj', 'conj:appos', 'conj:coord', 'conj:dicto', 'conj:emb', 'conj:obj', 'det', 'det:num', 'discourse', 'dislocated', 'flat', 'flat:foreign', 'flat:name', 'goeswith', 'list', 'mod', 'mod:appos', 'mod:emph', 'mod:num', 'mod:periph', 'mod:poss', 'orphan', 'parataxis', 'parataxis:conj', 'parataxis:discourse', 'parataxis:dislocated', 'parataxis:insert', 'parataxis:obj', 'parataxis:parenth', 'punct', 'reparandum', 'subj', 'total', 'udep', 'unk', 'vocative']\n",
      "{'appos': {}, 'cc': {}, 'clf': {}, 'comp': {}, 'comp:aux': {}, 'comp:cleft': {}, 'comp:obj': {}, 'comp:obl': {}, 'comp:pred': {}, 'compound': {}, 'compound:prt': {}, 'compound:redup': {}, 'compound:svc': {}, 'conj': {}, 'conj:appos': {}, 'conj:coord': {}, 'conj:dicto': {}, 'conj:emb': {}, 'conj:obj': {}, 'det': {}, 'det:num': {}, 'discourse': {}, 'dislocated': {}, 'flat': {}, 'flat:foreign': {}, 'flat:name': {}, 'goeswith': {}, 'list': {}, 'mod': {}, 'mod:appos': {}, 'mod:emph': {}, 'mod:num': {}, 'mod:periph': {}, 'mod:poss': {}, 'orphan': {}, 'parataxis': {}, 'parataxis:conj': {}, 'parataxis:discourse': {}, 'parataxis:dislocated': {}, 'parataxis:insert': {}, 'parataxis:obj': {}, 'parataxis:parenth': {}, 'punct': {}, 'reparandum': {}, 'subj': {}, 'total': {}, 'udep': {}, 'unk': {}, 'vocative': {}}\n"
     ]
    }
   ],
   "source": [
    "#prepare the dictionary info to combine information and to write in relevant files\n",
    "#print(colNamesDict['f.tsv'])\n",
    "info = {}\n",
    "for fl, colnames in colNamesDict.items():\n",
    "    info[fl]={}\n",
    "    for colname in colnames:\n",
    "        info[fl][colname]= {}\n",
    "\n",
    "print(info['f.tsv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f.tsv\n",
      "['Afrikaans', 'Assyrian', 'SouthLevantineArabic', 'Akkadian', 'Amharic', 'Apurinã', 'Akuntsu', 'Arabic', 'Belarusian', 'Bulgarian', 'Bhojpuri', 'Bambara', 'Breton', 'Buryat', 'Catalan', 'Chukot', 'Coptic', 'Czech', 'OldChurchSlavonic', 'Welsh', 'Danish', 'German', 'Greek', 'English', 'Spanish', 'Estonian', 'Basque', 'Persian', 'Finnish', 'Faroese', 'French', 'OldFrench', 'Irish', 'Gaelic', 'Galician', 'Gothic', 'AncientGreek', 'SwissGerman', 'MbyáGuaraní', 'Manx', 'Hebrew', 'Hindi', 'Croatian', 'UpperSorbian', 'Hungarian', 'Armenian', 'Indonesian', 'Icelandic', 'Italian', 'Japanese', 'Khunsari', 'Kazakh', 'Kurmanji', 'Korean', 'Komi-Permyak', 'Komi', 'Karelian', 'Latin', 'Lithuanian', 'Latvian', 'ClassicalChinese', 'Moksha', 'Marathi', 'Maltese', 'Mundurukú', 'Erzya', 'Dutch', 'Norwegian', 'Nayini', 'Livvi', 'OldEastSlavic', 'OldTurkish', 'Naija', 'Polish', 'Portuguese', 'HindiEnglish', 'TurkishGerman', 'Romanian', 'Russian', 'Sanskrit', 'Slovak', 'Slovenian', 'NorthSami', 'SkoltSami', 'Soi', 'Albanian', 'Serbian', 'Swedish', 'SwedishSign', 'Tamil', 'Telugu', 'Thai', 'Tagalog', 'Tupinambá', 'Turkish', 'Uyghur', 'Ukrainian', 'Urdu', 'Vietnamese', 'Warlpiri', 'Wolof', 'Yoruba', 'Cantonese', 'Chinese', 'Czech', 'German', 'Japanese', 'Russian']\n",
      "{'Afrikaans', 'Bambara', 'Faroese', 'AncientGreek', 'OldTurkish', 'Slovenian', 'Hungarian', 'Finnish', 'Irish', 'Urdu', 'Yoruba', 'Bhojpuri', 'MbyáGuaraní', 'German', 'HindiEnglish', 'Bulgarian', 'Karelian', 'Komi', 'Swedish', 'SwedishSign', 'SwissGerman', 'Norwegian', 'Nayini', 'Chinese', 'Danish', 'OldFrench', 'Naija', 'Ukrainian', 'Tamil', 'Maltese', 'Hebrew', 'Kurmanji', 'Cantonese', 'Italian', 'Manx', 'Coptic', 'Albanian', 'Assyrian', 'Czech', 'Latin', 'Akkadian', 'Wolof', 'Tagalog', 'TurkishGerman', 'Vietnamese', 'Erzya', 'Warlpiri', 'ClassicalChinese', 'Marathi', 'OldEastSlavic', 'Slovak', 'Galician', 'SouthLevantineArabic', 'Kazakh', 'Indonesian', 'Soi', 'Basque', 'NorthSami', 'Greek', 'Latvian', 'Mundurukú', 'Croatian', 'Portuguese', 'Korean', 'Chukot', 'Khunsari', 'Tupinambá', 'Belarusian', 'English', 'Japanese', 'Romanian', 'OldChurchSlavonic', 'Breton', 'Gothic', 'Lithuanian', 'Dutch', 'Uyghur', 'SkoltSami', 'Icelandic', 'Arabic', 'Buryat', 'Livvi', 'Gaelic', 'Serbian', 'Hindi', 'Amharic', 'Catalan', 'Komi-Permyak', 'Sanskrit', 'Akuntsu', 'Moksha', 'Apurinã', 'Thai', 'Polish', 'Spanish', 'Turkish', 'Welsh', 'Russian', 'Persian', 'Armenian', 'Estonian', 'Telugu', 'French', 'UpperSorbian'}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e7ade6b54981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlangNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlangNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlangNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlangNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f.tsv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'appos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#combine and store information in info\n",
    "#collect language names in each group of files (group by filename)\n",
    "\n",
    "langNames = {}\n",
    "\n",
    "for fl in filesname:\n",
    "    print(fl)\n",
    "    langNames[fl] = []\n",
    "    for i in range(len(folders)):\n",
    "        #print(folders[i])\n",
    "        with open(folders[i]+'/'+fl, 'r+',encoding=\"utf8\") as f:\n",
    "            lines= f.readlines() \n",
    "            cols = lines[0].strip().split('\\t')\n",
    "            \n",
    "            for line in lines[1:]:\n",
    "                line = line.strip().split('\\t')\n",
    "                #print(line)\n",
    "                lang = line[0]\n",
    "                langNames[fl].append(lang)\n",
    "                for idx, colname in enumerate(cols[1:]):\n",
    "                    info[fl][colname][lang]=line[idx+1]\n",
    "            #print(cols, '\\n')\n",
    "    assert(len(langNames[fl])== len(set(langNames[fl])))       \n",
    "print(info['f.tsv']['appos']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Afrikaans', 'Akkadian', 'Akuntsu', 'Albanian', 'Amharic', 'AncientGreek', 'Apurinã', 'Arabic', 'Armenian', 'Assyrian', 'Bambara', 'Basque', 'Belarusian', 'Bhojpuri', 'Breton', 'Bulgarian', 'Buryat', 'Cantonese', 'Catalan', 'Chinese', 'Chukot', 'ClassicalChinese', 'Coptic', 'Croatian', 'Czech', 'Czech', 'Danish', 'Dutch', 'English', 'Erzya', 'Estonian', 'Faroese', 'Finnish', 'French', 'Gaelic', 'Galician', 'German', 'German', 'Gothic', 'Greek', 'Hebrew', 'Hindi', 'HindiEnglish', 'Hungarian', 'Icelandic', 'Indonesian', 'Irish', 'Italian', 'Japanese', 'Japanese', 'Karelian', 'Kazakh', 'Khunsari', 'Komi', 'Komi-Permyak', 'Korean', 'Kurmanji', 'Latin', 'Latvian', 'Lithuanian', 'Livvi', 'Maltese', 'Manx', 'Marathi', 'MbyáGuaraní', 'Moksha', 'Mundurukú', 'Naija', 'Nayini', 'NorthSami', 'Norwegian', 'OldChurchSlavonic', 'OldEastSlavic', 'OldFrench', 'OldTurkish', 'Persian', 'Polish', 'Portuguese', 'Romanian', 'Russian', 'Russian', 'Sanskrit', 'Serbian', 'SkoltSami', 'Slovak', 'Slovenian', 'Soi', 'SouthLevantineArabic', 'Spanish', 'Swedish', 'SwedishSign', 'SwissGerman', 'Tagalog', 'Tamil', 'Telugu', 'Thai', 'Tupinambá', 'Turkish', 'TurkishGerman', 'Ukrainian', 'UpperSorbian', 'Urdu', 'Uyghur', 'Vietnamese', 'Warlpiri', 'Welsh', 'Wolof', 'Yoruba']\n",
      "['Afrikaans', 'Akkadian', 'Akuntsu', 'Albanian', 'Amharic', 'AncientGreek', 'Apurinã', 'Arabic', 'Armenian', 'Assyrian', 'Bambara', 'Basque', 'Belarusian', 'Bhojpuri', 'Breton', 'Bulgarian', 'Buryat', 'Cantonese', 'Catalan', 'Chinese', 'Chukot', 'ClassicalChinese', 'Coptic', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Erzya', 'Estonian', 'Faroese', 'Finnish', 'French', 'Gaelic', 'Galician', 'German', 'Gothic', 'Greek', 'Hebrew', 'Hindi', 'HindiEnglish', 'Hungarian', 'Icelandic', 'Indonesian', 'Irish', 'Italian', 'Japanese', 'Karelian', 'Kazakh', 'Khunsari', 'Komi', 'Komi-Permyak', 'Korean', 'Kurmanji', 'Latin', 'Latvian', 'Lithuanian', 'Livvi', 'Maltese', 'Manx', 'Marathi', 'MbyáGuaraní', 'Moksha', 'Mundurukú', 'Naija', 'Nayini', 'NorthSami', 'Norwegian', 'OldChurchSlavonic', 'OldEastSlavic', 'OldFrench', 'OldTurkish', 'Persian', 'Polish', 'Portuguese', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'SkoltSami', 'Slovak', 'Slovenian', 'Soi', 'SouthLevantineArabic', 'Spanish', 'Swedish', 'SwedishSign', 'SwissGerman', 'Tagalog', 'Tamil', 'Telugu', 'Thai', 'Tupinambá', 'Turkish', 'TurkishGerman', 'Ukrainian', 'UpperSorbian', 'Urdu', 'Uyghur', 'Vietnamese', 'Warlpiri', 'Welsh', 'Wolof', 'Yoruba']\n"
     ]
    }
   ],
   "source": [
    "print(len(langNames[fl]))\n",
    "print(len(set(langNames[fl])))\n",
    "print(sorted(langNames[fl]))\n",
    "print(sorted(set(langNames[fl])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Afrikaans', 'Assyrian', 'SouthLevantineArabic', 'Akkadian', 'Amharic', 'Apurinã', 'Akuntsu', 'Arabic', 'Belarusian', 'Bulgarian', 'Bhojpuri', 'Bambara', 'Breton', 'Buryat', 'Catalan', 'Chukot', 'Coptic', 'Czech', 'OldChurchSlavonic', 'Welsh', 'Danish', 'German', 'Greek', 'English', 'Spanish', 'Estonian', 'Basque', 'Persian', 'Finnish', 'Faroese', 'French', 'OldFrench', 'Irish', 'Gaelic', 'Galician', 'Gothic', 'AncientGreek', 'SwissGerman', 'MbyáGuaraní', 'Manx', 'Hebrew', 'Hindi', 'Croatian', 'UpperSorbian', 'Hungarian', 'Armenian', 'Indonesian', 'Icelandic', 'Italian', 'Japanese', 'Khunsari', 'Kazakh', 'Kurmanji', 'Korean', 'Komi-Permyak', 'Komi', 'Karelian', 'Latin', 'Lithuanian', 'Latvian', 'ClassicalChinese', 'Moksha', 'Marathi', 'Maltese', 'Mundurukú', 'Erzya', 'Dutch', 'Norwegian', 'Nayini', 'Livvi', 'OldEastSlavic', 'OldTurkish', 'Naija', 'Polish', 'Portuguese', 'HindiEnglish', 'TurkishGerman', 'Romanian', 'Russian', 'Sanskrit', 'Slovak', 'Slovenian', 'NorthSami', 'SkoltSami', 'Soi', 'Albanian', 'Serbian', 'Swedish', 'SwedishSign', 'Tamil', 'Telugu', 'Thai', 'Tagalog', 'Tupinambá', 'Turkish', 'Uyghur', 'Ukrainian', 'Urdu', 'Vietnamese', 'Warlpiri', 'Wolof', 'Yoruba', 'Cantonese', 'Chinese'])\n"
     ]
    }
   ],
   "source": [
    "print(info['f.tsv']['appos'].keys())\n",
    "#' '.join(info['f.tsv']['appos'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-865a31cee42b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-865a31cee42b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    with open(outputFolder+'/'+fl, 'w+',encoding=\"utf8\")as t:\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#write combined informations in files of output folder\n",
    "for fl in filesname:\n",
    "    print(fl)\n",
    "    with open(outputFolder+'/'+fl, 'w+',encoding=\"utf8\")as t:\n",
    "        line0 = \"name\"+'\\t'+'\\t'.join(info[fl].keys())\n",
    "        t.write(line0)\n",
    "        for lang in langNames[fl]:\n",
    "            line = lang\n",
    "            for col in colNamesDict[fl]:\n",
    "                line += '\\t'+info[fl].get(col,0)\n",
    "            t.write(line+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "with open('test.tsv','w+',encoding=\"utf8\")as t:\n",
    "    with open(\"ai.txt\",'r',encoding='utf8')as f:\n",
    "        # print(f.readlines())\n",
    "        for line in f.readlines():\n",
    "            line_list = line.strip('\\n') # supprimer le \\n\n",
    "            #print( type(line_list))\n",
    "            line = line_list[3:] #car la phrase commence a idx == 3\n",
    "            #print('\\n', line)\n",
    "            if(join):\n",
    "                tsv_list = [line]\n",
    "                join = not join\n",
    "            else:\n",
    "                tsv_list.append(line)\n",
    "                tsv_list = '\\t'.join(tsv_list)\n",
    "                t.write(tsv_list+'\\n')\n",
    "                join = not join\n",
    "    with open (\"ai.txt\",'r',encoding='utf8')as f:\n",
    "        for line in f.readlines():\n",
    "            line = ' '.join(sep_ponct(line)) \n",
    "            t.write(line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
